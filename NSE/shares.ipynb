{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b58e9841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/61] Downloading EGAD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johngitau/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved html/EGAD.html\n",
      "[2/61] Downloading KAPC...\n",
      "✅ Saved html/KAPC.html\n",
      "[3/61] Downloading KUKZ...\n",
      "✅ Saved html/KUKZ.html\n",
      "[4/61] Downloading LIMT...\n",
      "✅ Saved html/LIMT.html\n",
      "[5/61] Downloading SASN...\n",
      "✅ Saved html/SASN.html\n",
      "[6/61] Downloading WTK...\n",
      "✅ Saved html/WTK.html\n",
      "[7/61] Downloading CGEN...\n",
      "✅ Saved html/CGEN.html\n",
      "[8/61] Downloading ABSA...\n",
      "✅ Saved html/ABSA.html\n",
      "[9/61] Downloading SBIC...\n",
      "✅ Saved html/SBIC.html\n",
      "[10/61] Downloading IMH...\n",
      "✅ Saved html/IMH.html\n",
      "[11/61] Downloading DTK...\n",
      "✅ Saved html/DTK.html\n",
      "[12/61] Downloading SCBK...\n",
      "✅ Saved html/SCBK.html\n",
      "[13/61] Downloading EQTY...\n",
      "✅ Saved html/EQTY.html\n",
      "[14/61] Downloading COOP...\n",
      "✅ Saved html/COOP.html\n",
      "[15/61] Downloading BKG...\n",
      "✅ Saved html/BKG.html\n",
      "[16/61] Downloading HFCK...\n",
      "✅ Saved html/HFCK.html\n",
      "[17/61] Downloading KCB...\n",
      "✅ Saved html/KCB.html\n",
      "[18/61] Downloading NCBA...\n",
      "✅ Saved html/NCBA.html\n",
      "[19/61] Downloading XPRS...\n",
      "✅ Saved html/XPRS.html\n",
      "[20/61] Downloading SMER...\n",
      "✅ Saved html/SMER.html\n",
      "[21/61] Downloading KQ...\n",
      "✅ Saved html/KQ.html\n",
      "[22/61] Downloading NMG...\n",
      "✅ Saved html/NMG.html\n",
      "[23/61] Downloading SGL...\n",
      "✅ Saved html/SGL.html\n",
      "[24/61] Downloading TPSE...\n",
      "✅ Saved html/TPSE.html\n",
      "[25/61] Downloading SCAN...\n",
      "✅ Saved html/SCAN.html\n",
      "[26/61] Downloading UCHM...\n",
      "✅ Saved html/UCHM.html\n",
      "[27/61] Downloading LKL...\n",
      "✅ Saved html/LKL.html\n",
      "[28/61] Downloading NBV...\n",
      "✅ Saved html/NBV.html\n",
      "[29/61] Downloading BAMB...\n",
      "✅ Saved html/BAMB.html\n",
      "[30/61] Downloading CRWN...\n",
      "✅ Saved html/CRWN.html\n",
      "[31/61] Downloading CABL...\n",
      "✅ Saved html/CABL.html\n",
      "[32/61] Downloading PORT...\n",
      "✅ Saved html/PORT.html\n",
      "[33/61] Downloading TOTL...\n",
      "✅ Saved html/TOTL.html\n",
      "[34/61] Downloading KEGN...\n",
      "✅ Saved html/KEGN.html\n",
      "[35/61] Downloading KPLC...\n",
      "✅ Saved html/KPLC.html\n",
      "[36/61] Downloading UMME...\n",
      "✅ Saved html/UMME.html\n",
      "[37/61] Downloading JUB...\n",
      "✅ Saved html/JUB.html\n",
      "[38/61] Downloading SLAM...\n",
      "✅ Saved html/SLAM.html\n",
      "[39/61] Downloading KNRE...\n",
      "✅ Saved html/KNRE.html\n",
      "[40/61] Downloading LBTY...\n",
      "✅ Saved html/LBTY.html\n",
      "[41/61] Downloading BRIT...\n",
      "✅ Saved html/BRIT.html\n",
      "[42/61] Downloading CIC...\n",
      "✅ Saved html/CIC.html\n",
      "[43/61] Downloading OCH...\n",
      "✅ Saved html/OCH.html\n",
      "[44/61] Downloading CTUM...\n",
      "✅ Saved html/CTUM.html\n",
      "[45/61] Downloading TCL...\n",
      "✅ Saved html/TCL.html\n",
      "[46/61] Downloading HAFR...\n",
      "✅ Saved html/HAFR.html\n",
      "[47/61] Downloading KURV...\n",
      "✅ Saved html/KURV.html\n",
      "[48/61] Downloading NSE...\n",
      "✅ Saved html/NSE.html\n",
      "[49/61] Downloading BOC...\n",
      "✅ Saved html/BOC.html\n",
      "[50/61] Downloading BAT...\n",
      "✅ Saved html/BAT.html\n",
      "[51/61] Downloading CARB...\n",
      "✅ Saved html/CARB.html\n",
      "[52/61] Downloading EABL...\n",
      "✅ Saved html/EABL.html\n",
      "[53/61] Downloading UNGA...\n",
      "✅ Saved html/UNGA.html\n",
      "[54/61] Downloading EVRD...\n",
      "✅ Saved html/EVRD.html\n",
      "[55/61] Downloading AMAC...\n",
      "✅ Saved html/AMAC.html\n",
      "[56/61] Downloading FTGH...\n",
      "✅ Saved html/FTGH.html\n",
      "[57/61] Downloading SKL...\n",
      "✅ Saved html/SKL.html\n",
      "[58/61] Downloading SCOM...\n",
      "✅ Saved html/SCOM.html\n",
      "[59/61] Downloading LAPR...\n",
      "✅ Saved html/LAPR.html\n",
      "[60/61] Downloading GLD...\n",
      "✅ Saved html/GLD.html\n",
      "[61/61] Downloading SMWF...\n",
      "✅ Saved html/SMWF.html\n",
      "✅ Download step complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "import requests\n",
    "\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "from urllib3.util.retry import Retry\n",
    " \n",
    "# List of NSE tickers (add all you want here)\n",
    "\n",
    "tickers = [\n",
    "\n",
    "   \"EGAD\",\"KAPC\",\"KUKZ\",\"LIMT\",\"SASN\",\"WTK\",\n",
    "\"CGEN\",\n",
    "\"ABSA\",\"SBIC\",\"IMH\",\"DTK\",\"SCBK\",\"EQTY\",\"COOP\",\"BKG\",\"HFCK\",\"KCB\",\"NCBA\",\n",
    "\"XPRS\",\"SMER\",\"KQ\",\"NMG\",\"SGL\",\"TPSE\",\"SCAN\",\"UCHM\",\"LKL\",\"NBV\",\n",
    "\"BAMB\",\"CRWN\",\"CABL\",\"PORT\",\n",
    "\"TOTL\",\"KEGN\",\"KPLC\",\"UMME\",\n",
    "\"JUB\",\"SLAM\",\"KNRE\",\"LBTY\",\"BRIT\",\"CIC\",\n",
    "\"OCH\",\"CTUM\",\"TCL\",\"HAFR\",\"KURV\",\n",
    "\"NSE\",\n",
    "\"BOC\",\"BAT\",\"CARB\",\"EABL\",\"UNGA\",\"EVRD\",\"AMAC\",\"FTGH\",\"SKL\",\n",
    "\"SCOM\",\n",
    "\"LAPR\",\"GLD\",\"SMWF\"\n",
    "]\n",
    " \n",
    "BASE_URL = \"https://afx.kwayisi.org/nse/{}.html\"\n",
    " \n",
    "os.makedirs(\"html\", exist_ok=True)\n",
    " \n",
    "# Session with retries\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "retries = Retry(\n",
    "\n",
    "    total=5,\n",
    "\n",
    "    backoff_factor=2,\n",
    "\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "\n",
    "    allowed_methods=[\"GET\"]\n",
    "\n",
    ")\n",
    "\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "session.mount(\"http://\", adapter)\n",
    " \n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    " \n",
    "for i, ticker in enumerate(tickers, 1):\n",
    "\n",
    "    url = BASE_URL.format(ticker.lower())\n",
    "\n",
    "    print(f\"[{i}/{len(tickers)}] Downloading {ticker}...\")\n",
    " \n",
    "    try:\n",
    "\n",
    "        r = session.get(url, headers=HEADERS, timeout=30)\n",
    "\n",
    "        r.raise_for_status()\n",
    " \n",
    "        filename = f\"html/{ticker}.html\"\n",
    "\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            f.write(r.text)\n",
    " \n",
    "        print(f\"✅ Saved {filename}\")\n",
    " \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"⚠️ Failed to download {ticker}: {e}\")\n",
    " \n",
    "    time.sleep(3)  # be polite to the server\n",
    " \n",
    "print(\"✅ Download step complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "results = []\n",
    "history_rows = []\n",
    "html_folder = \"html\"\n",
    "\n",
    "\n",
    "def parse_number(s):\n",
    "    \"\"\"Convert strings like '1.32T', '158B', '1.89M', '623,465' to float.\"\"\"\n",
    "    if not s or s == \"—\":\n",
    "        return None\n",
    "    s = s.strip().replace(\",\", \"\")\n",
    "    multipliers = {\"T\": 1e12, \"B\": 1e9, \"M\": 1e6, \"K\": 1e3}\n",
    "    for suffix, mult in multipliers.items():\n",
    "        if s.upper().endswith(suffix):\n",
    "            try:\n",
    "                return float(s[:-1]) * mult\n",
    "            except ValueError:\n",
    "                return None\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_pct(s):\n",
    "    \"\"\"Convert '+12.8%' or '-1.45%' to float like 12.8 or -1.45.\"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    m = re.search(r\"([+-]?[\\d.]+)%\", s)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "for filename in sorted(os.listdir(html_folder)):\n",
    "    if not filename.endswith(\".html\"):\n",
    "        continue\n",
    "\n",
    "    ticker = filename.replace(\".html\", \"\")\n",
    "    filepath = os.path.join(html_folder, filename)\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "\n",
    "    # --- Current price & daily change ---\n",
    "    price_match = re.search(\n",
    "        rf\"{ticker}\\s*[•·]\\s*([\\d.]+)\\s*[▴▾▵▿]?\\s*([+-]?[\\d.]+)\\s*\\(([\\d.]+%)\\)\",\n",
    "        text, re.IGNORECASE,\n",
    "    )\n",
    "    current_price = float(price_match.group(1)) if price_match else None\n",
    "    daily_change = float(price_match.group(2)) if price_match else None\n",
    "    daily_change_pct = parse_pct(price_match.group(3)) if price_match else None\n",
    "\n",
    "    # --- Company name from <h1> ---\n",
    "    h1 = soup.find(\"h1\")\n",
    "    company_name = h1.text.split(\" - \", 1)[1].strip() if h1 and \" - \" in h1.text else None\n",
    "\n",
    "    # --- Live Trading Feed table ---\n",
    "    def get_table_value(label):\n",
    "        \"\"\"Find a <td> whose text matches label, return the next <td>'s text.\"\"\"\n",
    "        for td in soup.find_all(\"td\"):\n",
    "            if td.get_text(strip=True) == label:\n",
    "                nxt = td.find_next_sibling(\"td\")\n",
    "                if nxt:\n",
    "                    return nxt.get_text(strip=True)\n",
    "        return None\n",
    "\n",
    "    opening = parse_number(get_table_value(\"Opening Price\"))\n",
    "    day_low = parse_number(get_table_value(\"Day's Low Price\"))\n",
    "    day_high = parse_number(get_table_value(\"Day's High Price\"))\n",
    "    volume = parse_number(get_table_value(\"Traded Volume\"))\n",
    "    num_deals = parse_number(get_table_value(\"Number of Deals\"))\n",
    "    turnover = parse_number(get_table_value(\"Gross Turnover\"))\n",
    "\n",
    "    # --- Growth & Valuation table ---\n",
    "    eps = parse_number(get_table_value(\"Earnings Per Share\"))\n",
    "    pe_ratio = parse_number(get_table_value(\"Price/Earning Ratio\"))\n",
    "    dps = parse_number(get_table_value(\"Dividend Per Share\"))\n",
    "    div_yield_raw = get_table_value(\"Dividend Yield\")\n",
    "    div_yield = parse_pct(div_yield_raw) if div_yield_raw else None\n",
    "    shares_out = parse_number(get_table_value(\"Shares Outstanding\"))\n",
    "    market_cap = parse_number(get_table_value(\"Market Capitalization\"))\n",
    "\n",
    "    # --- Performance percentages (1WK, 4WK, 3MO, 6MO, 1YR, YTD) ---\n",
    "    perf_div = soup.find(\"div\", attrs={\"data-perf\": True})\n",
    "    perf = {}\n",
    "    if perf_div:\n",
    "        headers = [th.get(\"title\", th.text) for th in perf_div.find_all(\"th\")]\n",
    "        values = [td.get_text(strip=True) for td in perf_div.find_all(\"td\")]\n",
    "        for h, v in zip(headers, values):\n",
    "            perf[h] = parse_pct(v)\n",
    "\n",
    "    # --- Sector & Industry from factsheet ---\n",
    "    sector = industry = None\n",
    "    fact_div = soup.find(\"div\", attrs={\"data-fact\": True})\n",
    "    if fact_div:\n",
    "        for dt in fact_div.find_all(\"dt\"):\n",
    "            label = dt.get_text(strip=True)\n",
    "            dd = dt.find_next_sibling(\"dd\")\n",
    "            if dd:\n",
    "                if label == \"Sector\":\n",
    "                    sector = dd.get_text(strip=True)\n",
    "                elif label == \"Industry\":\n",
    "                    industry = dd.get_text(strip=True)\n",
    "\n",
    "    # --- Historical 10-day prices ---\n",
    "    hist_table = soup.find(\"table\", attrs={\"data-hist\": True})\n",
    "    if hist_table:\n",
    "        for tr in hist_table.find(\"tbody\").find_all(\"tr\"):\n",
    "            tds = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
    "            if len(tds) >= 3:\n",
    "                history_rows.append({\n",
    "                    \"Ticker\": ticker,\n",
    "                    \"Date\": tds[0],\n",
    "                    \"Volume\": parse_number(tds[1]),\n",
    "                    \"Close\": parse_number(tds[2]),\n",
    "                    \"Change\": parse_number(tds[3]) if len(tds) > 3 else None,\n",
    "                    \"Change%\": parse_pct(tds[4]) if len(tds) > 4 else None,\n",
    "                })\n",
    "\n",
    "    results.append({\n",
    "        \"Ticker\": ticker,\n",
    "        \"Company\": company_name,\n",
    "        \"Sector\": sector,\n",
    "        \"Industry\": industry,\n",
    "        \"Price\": current_price,\n",
    "        \"Change\": daily_change,\n",
    "        \"Change%\": daily_change_pct,\n",
    "        \"Open\": opening,\n",
    "        \"Low\": day_low,\n",
    "        \"High\": day_high,\n",
    "        \"Volume\": volume,\n",
    "        \"Deals\": num_deals,\n",
    "        \"Turnover\": turnover,\n",
    "        \"EPS\": eps,\n",
    "        \"P/E\": pe_ratio,\n",
    "        \"DPS\": dps,\n",
    "        \"Div Yield%\": div_yield,\n",
    "        \"Shares Out\": shares_out,\n",
    "        \"Market Cap\": market_cap,\n",
    "        \"1WK%\": perf.get(\"1-Week\"),\n",
    "        \"4WK%\": perf.get(\"4-Week\"),\n",
    "        \"3MO%\": perf.get(\"3-Month\"),\n",
    "        \"6MO%\": perf.get(\"6-Month\"),\n",
    "        \"1YR%\": perf.get(\"1-Year\"),\n",
    "        \"YTD%\": perf.get(\"Year-to-Date\"),\n",
    "    })\n",
    "\n",
    "    print(f\"Parsed {ticker}: KES {current_price} | YTD {perf.get('Year-to-Date', 'N/A')}%\")\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df_hist = pd.DataFrame(history_rows)\n",
    "df_hist[\"Date\"] = pd.to_datetime(df_hist[\"Date\"])\n",
    "\n",
    "df.to_csv(\"nse_snapshot.csv\", index=False)\n",
    "df_hist.to_csv(\"nse_history_10d.csv\", index=False)\n",
    "\n",
    "print(f\"\\nExtracted {len(df)} stocks, {len(df_hist)} historical rows\")\n",
    "print(\"Saved: nse_snapshot.csv, nse_history_10d.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7j0ksytkrn2",
   "metadata": {},
   "source": [
    "## Accumulate Historical Data\n",
    "\n",
    "Pivots the 10-day history from each HTML scrape into date-stamped columns (`Close_2026-02-11`, `Volume_2026-02-11`, ...).  \n",
    "Merges with existing `nse_historical.csv` so data accumulates across runs — old dates are preserved, new dates are appended as columns.  \n",
    "Run this notebook daily to build up your own price history over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tqfv13t27y9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIST_FILE = \"nse_historical.csv\"\n",
    "\n",
    "# --- Build wide-format columns from the scraped 10-day history ---\n",
    "# df_hist has: Ticker, Date, Volume, Close, Change, Change%\n",
    "new_wide = df_hist.copy()\n",
    "new_wide[\"DateStr\"] = new_wide[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Pivot Close prices: one column per date\n",
    "close_pivot = new_wide.pivot(index=\"Ticker\", columns=\"DateStr\", values=\"Close\")\n",
    "close_pivot.columns = [f\"Close_{d}\" for d in close_pivot.columns]\n",
    "\n",
    "# Pivot Volumes: one column per date\n",
    "vol_pivot = new_wide.pivot(index=\"Ticker\", columns=\"DateStr\", values=\"Volume\")\n",
    "vol_pivot.columns = [f\"Volume_{d}\" for d in vol_pivot.columns]\n",
    "\n",
    "# Combine into one wide DataFrame\n",
    "new_data = close_pivot.join(vol_pivot)\n",
    "\n",
    "# Add static info columns from main df\n",
    "static_cols = df.set_index(\"Ticker\")[[\"Company\", \"Sector\", \"Industry\"]]\n",
    "new_data = static_cols.join(new_data)\n",
    "new_data.index.name = \"Ticker\"\n",
    "\n",
    "# --- Merge with existing historical file ---\n",
    "if os.path.exists(HIST_FILE):\n",
    "    existing = pd.read_csv(HIST_FILE, index_col=\"Ticker\")\n",
    "    print(f\"Loaded existing {HIST_FILE}: {existing.shape[0]} tickers, {len(existing.columns)} columns\")\n",
    "\n",
    "    # Find date columns already in existing file\n",
    "    existing_date_cols = [c for c in existing.columns if c.startswith(\"Close_\") or c.startswith(\"Volume_\")]\n",
    "    new_date_cols = [c for c in new_data.columns if c.startswith(\"Close_\") or c.startswith(\"Volume_\")]\n",
    "\n",
    "    # New dates that don't exist yet\n",
    "    truly_new = [c for c in new_date_cols if c not in existing.columns]\n",
    "\n",
    "    # Update: overwrite new data columns into existing, keep old ones intact\n",
    "    for col in new_date_cols:\n",
    "        existing[col] = new_data[col]\n",
    "\n",
    "    # Also update static columns in case a new ticker was added\n",
    "    for col in [\"Company\", \"Sector\", \"Industry\"]:\n",
    "        existing[col] = new_data[col].combine_first(existing[col]) if col in existing.columns else new_data[col]\n",
    "\n",
    "    # Add any new tickers that weren't in the file before\n",
    "    new_tickers = new_data.index.difference(existing.index)\n",
    "    if len(new_tickers) > 0:\n",
    "        existing = pd.concat([existing, new_data.loc[new_tickers]])\n",
    "\n",
    "    merged = existing\n",
    "    print(f\"Added {len(truly_new)} new date columns: {truly_new[:5]}{'...' if len(truly_new) > 5 else ''}\")\n",
    "else:\n",
    "    merged = new_data\n",
    "    print(f\"No existing {HIST_FILE} found — creating new file\")\n",
    "\n",
    "# --- Sort columns: static first, then date columns in chronological order ---\n",
    "static = [\"Company\", \"Sector\", \"Industry\"]\n",
    "date_cols = sorted([c for c in merged.columns if c not in static])\n",
    "merged = merged[static + date_cols]\n",
    "\n",
    "# Save\n",
    "merged.to_csv(HIST_FILE)\n",
    "\n",
    "# Summary\n",
    "close_dates = sorted(set(c.replace(\"Close_\", \"\") for c in merged.columns if c.startswith(\"Close_\")))\n",
    "print(f\"\\nSaved {HIST_FILE}: {merged.shape[0]} tickers x {len(close_dates)} unique dates\")\n",
    "print(f\"Date range: {close_dates[0]} to {close_dates[-1]}\")\n",
    "print(f\"Total columns: {len(merged.columns)}\")\n",
    "print(f\"\\nRun this notebook again tomorrow to add more dates!\")\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g9jhhbbx1g",
   "metadata": {},
   "source": [
    "## Momentum & Value Analysis\n",
    "\n",
    "Ranks stocks by short-term momentum (1WK, 4WK) and longer-term trend (3MO, YTD).  \n",
    "Calculates intraday volatility range and flags potential buy/sell signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ojdlxy1c6gj",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "analysis = df[[\"Ticker\", \"Company\", \"Sector\", \"Price\", \"Open\", \"Low\", \"High\",\n",
    "               \"Volume\", \"Turnover\", \"Market Cap\",\n",
    "               \"1WK%\", \"4WK%\", \"3MO%\", \"6MO%\", \"1YR%\", \"YTD%\"]].copy()\n",
    "\n",
    "# --- Intraday range as % of price (volatility proxy) ---\n",
    "analysis[\"Intraday Range%\"] = np.where(\n",
    "    analysis[\"Price\"] > 0,\n",
    "    ((analysis[\"High\"] - analysis[\"Low\"]) / analysis[\"Price\"] * 100).round(2),\n",
    "    None,\n",
    ")\n",
    "\n",
    "# --- Momentum score: weighted average of performance periods ---\n",
    "# Higher weight on recent periods for trading signals\n",
    "analysis[\"Momentum Score\"] = (\n",
    "    analysis[\"1WK%\"].fillna(0) * 0.30 +\n",
    "    analysis[\"4WK%\"].fillna(0) * 0.25 +\n",
    "    analysis[\"3MO%\"].fillna(0) * 0.20 +\n",
    "    analysis[\"YTD%\"].fillna(0) * 0.15 +\n",
    "    analysis[\"6MO%\"].fillna(0) * 0.10\n",
    ").round(2)\n",
    "\n",
    "# --- Rank by momentum ---\n",
    "analysis[\"Momentum Rank\"] = analysis[\"Momentum Score\"].rank(ascending=False).astype(int)\n",
    "\n",
    "# --- Simple signal based on momentum alignment ---\n",
    "def get_signal(row):\n",
    "    wk1 = row[\"1WK%\"] or 0\n",
    "    wk4 = row[\"4WK%\"] or 0\n",
    "    mo3 = row[\"3MO%\"] or 0\n",
    "    ytd = row[\"YTD%\"] or 0\n",
    "\n",
    "    # Strong buy: all timeframes positive and accelerating (1WK > 4WK avg)\n",
    "    if wk1 > 0 and wk4 > 0 and mo3 > 0 and ytd > 0 and wk1 > (wk4 / 4):\n",
    "        return \"STRONG BUY\"\n",
    "    # Buy: mostly positive trend\n",
    "    elif wk1 > 0 and wk4 > 0 and mo3 > 0:\n",
    "        return \"BUY\"\n",
    "    # Hold: mixed signals\n",
    "    elif wk1 > 0 or wk4 > 0:\n",
    "        return \"HOLD\"\n",
    "    # Sell: negative short-term momentum\n",
    "    elif wk1 < 0 and wk4 < 0:\n",
    "        return \"SELL\"\n",
    "    else:\n",
    "        return \"HOLD\"\n",
    "\n",
    "analysis[\"Signal\"] = analysis.apply(get_signal, axis=1)\n",
    "\n",
    "# Sort by momentum rank\n",
    "analysis = analysis.sort_values(\"Momentum Rank\")\n",
    "\n",
    "# Display key columns\n",
    "display_cols = [\"Momentum Rank\", \"Ticker\", \"Company\", \"Price\", \"1WK%\", \"4WK%\",\n",
    "                \"3MO%\", \"YTD%\", \"Momentum Score\", \"Intraday Range%\", \"Signal\"]\n",
    "print(\"=\" * 80)\n",
    "print(\"NSE MOMENTUM DASHBOARD - Sorted by Momentum Score\")\n",
    "print(\"=\" * 80)\n",
    "analysis[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5340277anu6",
   "metadata": {},
   "source": [
    "## 10-Day Price Trends & Volume Analysis\n",
    "\n",
    "Shows which stocks are trending up/down over the last 10 trading days and highlights unusual volume spikes (potential breakout signals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9kzv4r8li2r",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 10-day price change for each stock ---\n",
    "trend_summary = []\n",
    "for ticker, grp in df_hist.groupby(\"Ticker\"):\n",
    "    grp = grp.sort_values(\"Date\")\n",
    "    if len(grp) >= 2:\n",
    "        oldest_close = grp.iloc[0][\"Close\"]\n",
    "        latest_close = grp.iloc[-1][\"Close\"]\n",
    "        pct_10d = ((latest_close - oldest_close) / oldest_close * 100) if oldest_close else 0\n",
    "        avg_vol = grp[\"Volume\"].mean()\n",
    "        max_vol = grp[\"Volume\"].max()\n",
    "        vol_spike = max_vol / avg_vol if avg_vol else 0  # >2 = unusual spike\n",
    "        positive_days = (grp[\"Change\"].fillna(0) > 0).sum()\n",
    "        negative_days = (grp[\"Change\"].fillna(0) < 0).sum()\n",
    "        trend_summary.append({\n",
    "            \"Ticker\": ticker,\n",
    "            \"10D Change%\": round(pct_10d, 2),\n",
    "            \"Avg Volume\": int(avg_vol),\n",
    "            \"Max Volume\": int(max_vol),\n",
    "            \"Vol Spike Ratio\": round(vol_spike, 1),\n",
    "            \"Up Days\": int(positive_days),\n",
    "            \"Down Days\": int(negative_days),\n",
    "            \"Win Rate%\": round(positive_days / len(grp) * 100, 1),\n",
    "        })\n",
    "\n",
    "df_trend = pd.DataFrame(trend_summary).sort_values(\"10D Change%\", ascending=False)\n",
    "print(\"=\" * 70)\n",
    "print(\"10-DAY TREND SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Vol Spike Ratio > 2.0 = unusual volume (possible breakout)\")\n",
    "print()\n",
    "display(df_trend)\n",
    "\n",
    "# --- Plot: 10-day price trend for top 6 movers ---\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "top_movers = df_trend.head(6)[\"Ticker\"].tolist()\n",
    "\n",
    "for i, ticker in enumerate(top_movers):\n",
    "    grp = df_hist[df_hist[\"Ticker\"] == ticker].sort_values(\"Date\")\n",
    "    ax = axes[i]\n",
    "    ax.plot(grp[\"Date\"], grp[\"Close\"], marker=\"o\", linewidth=2)\n",
    "    ax.fill_between(grp[\"Date\"], grp[\"Close\"], alpha=0.2)\n",
    "    ax.set_title(f\"{ticker} (10D: {df_trend[df_trend['Ticker']==ticker]['10D Change%'].values[0]:+.1f}%)\")\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Top 6 Movers - 10 Day Price Trend\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"top_movers_10d.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: top_movers_10d.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eqqopj7vdxa",
   "metadata": {},
   "source": [
    "## Sector Breakdown & Market Overview\n",
    "\n",
    "Market cap distribution by sector, and a final summary of top picks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9nmd9drdef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sector breakdown ---\n",
    "sector_summary = df.groupby(\"Sector\").agg(\n",
    "    Stocks=(\"Ticker\", \"count\"),\n",
    "    Total_MCap=(\"Market Cap\", \"sum\"),\n",
    "    Avg_YTD=(\"YTD%\", \"mean\"),\n",
    "    Avg_1WK=(\"1WK%\", \"mean\"),\n",
    ").round(2)\n",
    "sector_summary[\"Total_MCap_B\"] = (sector_summary[\"Total_MCap\"] / 1e9).round(1)\n",
    "sector_summary = sector_summary.sort_values(\"Total_MCap\", ascending=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SECTOR BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "display(sector_summary[[\"Stocks\", \"Total_MCap_B\", \"Avg_YTD\", \"Avg_1WK\"]])\n",
    "\n",
    "# --- Pie chart of market cap by sector ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sector_mcap = df.groupby(\"Sector\")[\"Market Cap\"].sum().sort_values(ascending=False)\n",
    "ax1.pie(sector_mcap, labels=sector_mcap.index, autopct=\"%1.0f%%\", startangle=90)\n",
    "ax1.set_title(\"Market Cap by Sector\")\n",
    "\n",
    "# --- Bar chart: YTD% by stock ---\n",
    "ytd = df[[\"Ticker\", \"YTD%\"]].dropna().sort_values(\"YTD%\", ascending=True)\n",
    "colors = [\"green\" if v > 0 else \"red\" for v in ytd[\"YTD%\"]]\n",
    "ax2.barh(ytd[\"Ticker\"], ytd[\"YTD%\"], color=colors)\n",
    "ax2.set_xlabel(\"YTD %\")\n",
    "ax2.set_title(\"Year-to-Date Performance\")\n",
    "ax2.axvline(x=0, color=\"black\", linewidth=0.5)\n",
    "ax2.grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"nse_overview.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# --- Final top picks ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOP PICKS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "strong_buys = analysis[analysis[\"Signal\"] == \"STRONG BUY\"][[\"Ticker\", \"Company\", \"Price\", \"Momentum Score\", \"1WK%\", \"YTD%\"]]\n",
    "buys = analysis[analysis[\"Signal\"] == \"BUY\"][[\"Ticker\", \"Company\", \"Price\", \"Momentum Score\", \"1WK%\", \"YTD%\"]]\n",
    "\n",
    "if len(strong_buys) > 0:\n",
    "    print(\"\\nSTRONG BUY signals:\")\n",
    "    display(strong_buys)\n",
    "if len(buys) > 0:\n",
    "    print(\"\\nBUY signals:\")\n",
    "    display(buys)\n",
    "\n",
    "sells = analysis[analysis[\"Signal\"] == \"SELL\"][[\"Ticker\", \"Company\", \"Price\", \"Momentum Score\", \"1WK%\", \"YTD%\"]]\n",
    "if len(sells) > 0:\n",
    "    print(\"\\nSELL signals (negative momentum across timeframes):\")\n",
    "    display(sells)\n",
    "\n",
    "# --- Volume leaders (most liquid = easier to trade) ---\n",
    "print(\"\\nMost Liquid Stocks (by turnover - easiest to buy/sell):\")\n",
    "liquidity = df.nlargest(5, \"Turnover\")[[\"Ticker\", \"Company\", \"Price\", \"Turnover\", \"Volume\", \"Deals\"]]\n",
    "display(liquidity)\n",
    "\n",
    "print(\"\\nNote: These signals are based on momentum trends only.\")\n",
    "print(\"Always consider fundamentals (EPS, P/E, dividends) before trading.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
